{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IgwgEBuXX_ou"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wq4wN3u3ZJj1"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    assert config.n_embd % config.n_head == 0\n",
    "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # matrix 3 times as large so it can be broken into QKV\n",
    "    self.c_proj = nn.Linear(config.n_embd, config.n_embd) # output projection\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embd = config.n_embd\n",
    "    # mask to attend to only tokens occuring previously to the current token\n",
    "    # only needed for normal attention implementation\n",
    "    #self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size() # batch size, seqlen, embedding size\n",
    "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # split into Q, K, V\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # attention mechanism to create matrix (TxT) for all queries and keys\n",
    "    # att = (q @ k.transpose(-2, -1)) * 1.0/math.sqrt(k.size(-1)) # (B, nh, T, T) (normalized by hs = embd_size / nheads)\n",
    "    # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # (B, nh, T, T) (bias registered from before to be lower triangular)\n",
    "    # att = F.softmax(att, dim=-1) # softmax op\n",
    "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "    # Flash Attention Implementation instead of above normal attention for speedup\n",
    "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # (B, nh, T, hs)\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # concat all the head outputs together (B, T, C)\n",
    "    # out proj\n",
    "    y = self.c_proj(y)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FrULTq6FddAe"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # project to a higher space to be able to learn more features\n",
    "    self.gelu = nn.GELU(approximate=\"tanh\") # approximate w/ tanh b/c originally GELU calc in TF was slow\n",
    "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd) # project back to the embedding layer\n",
    "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.c_fc(x)\n",
    "    x = self.gelu(x)\n",
    "    x = self.c_proj(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pU0vDgfxbwws"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "      self.attn = CausalSelfAttention(config)\n",
    "      self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "      self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "      # have the x + in each step because of residual connections\n",
    "      x = x + self.attn(self.ln_1(x)) # attention (reduce where each token shares information)\n",
    "      x = x + self.mlp(self.ln_2(x)) # map (each token is individually processed)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7XdHJyj7oT9Z"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # sequence length\n",
    "    vocab_size: int = 50257 # vocab size (num tokens)\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dRPdCcDoY550"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # positional embeddings\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # for the hidden layers\n",
    "            ln_f = nn.LayerNorm(config.n_embd) # layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # output projection to the vocab size\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight # weight tying\n",
    "\n",
    "        # applies the _init_weights function to all the sub modules of this module\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "      # weights according to the GPT2 implementation\n",
    "      if isinstance(module, nn.Linear):\n",
    "        std = 0.02\n",
    "        # layers in the MLP and CausalSelfAttention will have this attribute (as these layers contribute to\n",
    "        # the residual stream)\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "          if module.NANOGPT_SCALE_INIT:\n",
    "            # 2 times number of layers of residual streams because each block has MLP and Attention\n",
    "            # contributing to the residual stream\n",
    "            std *= (2 * self.config.n_layer)**-0.5\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=std) # weight normal dist with std .02\n",
    "        if module.bias is not None: # check if the layer has a bias term\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "      # idx is the token indices\n",
    "      B, T = idx.size()\n",
    "      # make sure prompt seqlen less than or equal to max model seq length\n",
    "      assert T <= self.config.block_size, f\"Cannot forward, index has length {T}, block size is {self.config.block_size}\"\n",
    "      pos = torch.arange(0, T, dtype=torch.long, device = idx.device) #shape (T) (makes sure input is on correct device)\n",
    "      pos_emb = self.transformer.wpe(pos) # shape (T, n_embd)\n",
    "      token_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
    "      x = token_emb + pos_emb # broadcasting done to be able to add these matrices\n",
    "\n",
    "      # forward the blocks of the transformers\n",
    "      for block in self.transformer.h:\n",
    "        x = block(x)\n",
    "\n",
    "      # layernorm and classifier\n",
    "      x = self.transformer.ln_f(x)\n",
    "      logits = self.lm_head(x)\n",
    "      loss = None\n",
    "\n",
    "      if targets is not None:\n",
    "        # need to flatten the matrices\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "      return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "      # collect all the parameters and its tensors, and then filter for the ones\n",
    "      # that require gradients\n",
    "      param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "      param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "      # only parameters that have >= 2 dimensions need to be weight decayed\n",
    "      # weight decay is used for regularization and preventing overfitting similar to L2 Regularization\n",
    "      decay_params = [p for n, p in param_dict.items() if p.dim() >= 2] # >= 2 to make sure its only weights that are decayed and not bias\n",
    "      nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "      # create a list with the weights that need to be decayed and others that don't\n",
    "      optim_groups = [\n",
    "          {'params': decay_params, 'weight_decay': weight_decay},\n",
    "          {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "      ]\n",
    "\n",
    "      num_decay_params = sum(p.numel() for p in decay_params)\n",
    "      num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "      print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "      print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "      #Create AdamW optimizer and use the fused version if it is available\n",
    "      # kernel fusion to be more efficient instead of iterating over all the tensors\n",
    "      fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "      use_fused = fused_available and device == \"cuda\"\n",
    "      print(f\"using fused AdamW: {use_fused}\")\n",
    "      optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "      return optimizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5eEfBssw_Id",
    "outputId": "9d1309f4-f1fc-462c-b4ea-9039a30b2047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZcSeMvWlljn",
    "outputId": "6a53b8e2-b970-42c6-a140-c490707659e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-21 23:06:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-11-21 23:06:17 (26.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "text = text[:1000]\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "poFcXmSls93o"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "class DataLoaderLite:\n",
    "  def __init__(self, B, T, process_rank, num_processes):\n",
    "    self.B = B\n",
    "    self.T = T\n",
    "    self.process_rank = process_rank\n",
    "    self.num_processes = num_processes\n",
    "\n",
    "    # at initialization load tokens from disc and store them into memory\n",
    "    with open(\"input.txt\", \"r\") as f:\n",
    "      text = f.read()\n",
    "\n",
    "    # encode the text into tokens\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(text)\n",
    "    self.tokens = torch.tensor(tokens)\n",
    "\n",
    "    print(f\"loaded {len(self.tokens)} tokens\")\n",
    "    print(f\"1 epoch = {len(self.tokens) // (self.B * self.T)} batches\")\n",
    "\n",
    "    # state\n",
    "    self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "  def next_batch(self):\n",
    "    B, T = self.B, self.T\n",
    "    buf = self.tokens[self.current_position: self.current_position + B*T + 1]\n",
    "    # update current position\n",
    "    self.current_position += B*T * self.num_processes\n",
    "    # get the x and y\n",
    "    x = buf[:-1].view(B, T) # get everything but the last token\n",
    "    y = buf[1:].view(B, T) # get correct next tokens\n",
    "\n",
    "    # wrap back around if next batch results in OOB\n",
    "    if self.current_position + B*T*self.num_processes + 1 > len(self.tokens):\n",
    "      self.current_position = B*T * self.num_processes\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoCzsrhX5yGk",
    "outputId": "d6c63d5a-d8a0-4263-e809-deee15e8479b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton\n",
      "Successfully installed triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D15Gf-PB6CbY"
   },
   "outputs": [],
   "source": [
    "import triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuD1B0lslgZY",
    "outputId": "fb9ea44c-a581-43ac-d193-2564b613620b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size 524288\n",
      "gradient accumulation steps 64\n",
      "loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0 loss 10.938887 time 155.32537126541138 lr 1.2000e-04 norm 27.0152\n",
      "step 1 loss 9.593569 time 127.2419023513794 lr 2.4000e-04 norm 7.5646\n",
      "step 2 loss 9.076535 time 127.14698219299316 lr 3.6000e-04 norm 2.3658\n",
      "step 3 loss 9.117402 time 127.20558905601501 lr 4.8000e-04 norm 5.1234\n",
      "step 4 loss 8.451342 time 126.84460926055908 lr 6.0000e-04 norm 2.3033\n",
      "step 5 loss 8.005286 time 126.78626203536987 lr 6.0000e-04 norm 1.9718\n",
      "step 6 loss 7.592834 time 126.73928022384644 lr 5.9934e-04 norm 1.7762\n",
      "step 7 loss 7.237749 time 127.0109224319458 lr 5.9737e-04 norm 1.4863\n",
      "step 8 loss 6.896439 time 126.99690842628479 lr 5.9410e-04 norm 1.1192\n",
      "step 9 loss 6.655927 time 127.00219416618347 lr 5.8954e-04 norm 1.1407\n",
      "step 10 loss 6.499631 time 127.00564408302307 lr 5.8372e-04 norm 1.8061\n",
      "step 11 loss 6.357316 time 126.99663925170898 lr 5.7666e-04 norm 1.3009\n",
      "step 12 loss 6.310370 time 127.06050062179565 lr 5.6840e-04 norm 1.7647\n",
      "step 13 loss 6.237617 time 126.89415502548218 lr 5.5897e-04 norm 1.5044\n",
      "step 14 loss 6.198271 time 127.08766746520996 lr 5.4843e-04 norm 0.9454\n",
      "step 15 loss 6.183991 time 127.11401438713074 lr 5.3683e-04 norm 0.4695\n",
      "step 16 loss 6.199239 time 127.16998314857483 lr 5.2422e-04 norm 2.4069\n",
      "step 17 loss 6.194416 time 127.02255630493164 lr 5.1067e-04 norm 1.4619\n",
      "step 18 loss 6.154597 time 127.03921437263489 lr 4.9623e-04 norm 0.6874\n",
      "step 19 loss 6.135531 time 127.12328672409058 lr 4.8098e-04 norm 1.0731\n",
      "step 20 loss 6.115943 time 127.08294224739075 lr 4.6500e-04 norm 1.1003\n",
      "step 21 loss 6.069847 time 127.3113214969635 lr 4.4836e-04 norm 0.6331\n",
      "step 22 loss 6.063158 time 127.197340965271 lr 4.3114e-04 norm 0.5999\n",
      "step 23 loss 6.025049 time 127.36140179634094 lr 4.1343e-04 norm 0.7737\n",
      "step 24 loss 6.002160 time 127.23516488075256 lr 3.9532e-04 norm 0.6223\n",
      "step 25 loss 5.993635 time 127.39485907554626 lr 3.7689e-04 norm 0.4348\n",
      "step 26 loss 5.974072 time 127.18792748451233 lr 3.5822e-04 norm 0.5983\n",
      "step 27 loss 5.987362 time 127.43478155136108 lr 3.3942e-04 norm 0.6433\n",
      "step 28 loss 5.961510 time 127.40104341506958 lr 3.2058e-04 norm 0.3967\n",
      "step 29 loss 5.955380 time 127.50184559822083 lr 3.0178e-04 norm 0.4113\n",
      "step 30 loss 5.955552 time 127.46881127357483 lr 2.8311e-04 norm 0.4212\n",
      "step 31 loss 5.937327 time 127.6879050731659 lr 2.6468e-04 norm 0.3678\n",
      "step 32 loss 5.948648 time 127.64818835258484 lr 2.4657e-04 norm 0.3130\n",
      "step 33 loss 5.921139 time 127.61309814453125 lr 2.2886e-04 norm 0.3380\n",
      "step 34 loss 5.912148 time 127.60935068130493 lr 2.1164e-04 norm 0.2412\n",
      "step 35 loss 5.913970 time 127.65085291862488 lr 1.9500e-04 norm 0.1967\n",
      "step 36 loss 5.896150 time 127.56803011894226 lr 1.7902e-04 norm 0.2159\n",
      "step 37 loss 5.910755 time 127.52320289611816 lr 1.6377e-04 norm 0.2018\n",
      "step 38 loss 5.882634 time 127.79903769493103 lr 1.4933e-04 norm 0.2534\n",
      "step 39 loss 5.872900 time 127.6683897972107 lr 1.3578e-04 norm 0.2754\n",
      "step 40 loss 5.874935 time 127.60974669456482 lr 1.2317e-04 norm 0.3211\n",
      "step 41 loss 5.859771 time 127.55509352684021 lr 1.1157e-04 norm 0.4598\n",
      "step 42 loss 5.875088 time 127.69123554229736 lr 1.0103e-04 norm 0.4225\n",
      "step 43 loss 5.848135 time 127.67729187011719 lr 9.1604e-05 norm 0.2738\n",
      "step 44 loss 5.842469 time 127.80982327461243 lr 8.3343e-05 norm 0.2539\n",
      "step 45 loss 5.845802 time 127.84902215003967 lr 7.6283e-05 norm 0.2441\n",
      "step 46 loss 5.830500 time 127.69928693771362 lr 7.0459e-05 norm 0.2693\n",
      "step 47 loss 5.848092 time 127.74769711494446 lr 6.5900e-05 norm 0.2330\n",
      "step 48 loss 5.823961 time 127.6603946685791 lr 6.2628e-05 norm 0.2299\n",
      "step 49 loss 5.820882 time 127.69247555732727 lr 6.0658e-05 norm 0.1868\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Launch the script with (torchrun --standalone --nproc_per_node=8 train_gpt2.py)\n",
    "\n",
    "# set up DDP (distributed data parallel)\n",
    "# use torchrun to set RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get(\"RANK\", -1)) != -1\n",
    "if ddp:\n",
    "  assert torch.cuda.is_available(), \"DDP requires CUDA\"\n",
    "  init_process_group(backend=\"nccl\")\n",
    "  ddp_rank = int(os.environ[\"RANK\"])\n",
    "  ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "  ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "  device = f\"cuda:{ddp_local_rank}\"\n",
    "  torch.cuda.set_device(device)\n",
    "  master_process = ddp_rank == 0 # master process chose randomly to be 0 for logging etc\n",
    "else:\n",
    "  # single process run\n",
    "  master_process = True\n",
    "  ddp_rank = 0\n",
    "  ddp_local_rank = 0\n",
    "  ddp_world_size = 1\n",
    "  device = \"cpu\"\n",
    "  if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(1337)\n",
    "\n",
    "total_batch_size = 524288 # ~.5M 2**19\n",
    "B = 8\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size divisibel by B * T * ddp_world_size\"\n",
    "# allows simulation of larger batch sizes without the memory restrictions\n",
    "# will end up doing grad_accum_steps number of forward and backward passes for each step\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "\n",
    "# print once\n",
    "if master_process:\n",
    "  print(f\"total desired batch size {total_batch_size}\")\n",
    "  print(f\"gradient accumulation steps {grad_accum_steps}\")\n",
    "\n",
    "#print(\"I am DDP rank \", ddp_rank)\n",
    "#import sys; sys.exit(0)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, process_rank=ddp_rank, num_processes=ddp_world_size)\n",
    "\n",
    "# TF32, will save some memory -- Not available on Tesla T4 GPU\n",
    "#torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "#model = GPT.from_pretrained('gpt2')\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device) # move tensors to device\n",
    "# compile model, take longer to compile but execution time sped up\n",
    "\"\"\"\n",
    "Compiles model to see what operations need to be run and can run the code\n",
    "efficiently. Implements kernel fusion to minimize the number of operations.\n",
    "\"\"\"\n",
    "model = torch.compile(model)\n",
    "\n",
    "if ddp:\n",
    "  model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "raw_model = model.module if ddp else model\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * .1 # 10% of the max according to paper\n",
    "warmup_steps = 5\n",
    "max_steps = 50\n",
    "\n",
    "def get_lr(step):\n",
    "  # linear increase for warmup steps\n",
    "  if step < warmup_steps:\n",
    "    return max_lr * (step + 1) / warmup_steps\n",
    "  if step > max_steps:\n",
    "    return min_lr\n",
    "\n",
    "  # cosine decary to the min_lr\n",
    "  # value between 0 - 1 because normalizing the steps\n",
    "  decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "  assert 0 <= decay_ratio <= 1\n",
    "\n",
    "  coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "  return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "\n",
    "# create an optimizer for the loss\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "# optimize loop\n",
    "for step in range(max_steps):\n",
    "  # get the next batch\n",
    "  t0 = time.time()\n",
    "\n",
    "\n",
    "  # always need to reset optimizer at the beginning\n",
    "  optimizer.zero_grad()\n",
    "  loss_accum = 0.0\n",
    "  for micro_step in range(grad_accum_steps):\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device) # move tensors from cpu to device\n",
    "    y = y.to(device)\n",
    "    # cast logits to be bfloat16 (going to change tensors)\n",
    "    #with torch.autocast(device_type=device, dtype=torch.bfloat16): (not supported on Tesla T4 GPU)\n",
    "    # calculate logits and the loss\n",
    "    logits, loss = model(x, y)\n",
    "    loss /= grad_accum_steps # normalize the loss\n",
    "    loss_accum += loss.detach()\n",
    "    # backwards step to calculate gradients (+=)\n",
    "    if ddp:\n",
    "      model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # only do the sharing btwn processes on last iteration\n",
    "    loss.backward() # gradients will continue to add up b/c .backward() always does a +=\n",
    "\n",
    "    if ddp:\n",
    "      dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "\n",
    "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients to prevent exploding gradients\n",
    "\n",
    "  lr = get_lr(step)\n",
    "  # optimizer might have more than one param other than lr\n",
    "  # iterate through the params and update the lr\n",
    "  for p in optimizer.param_groups:\n",
    "    p['lr'] = lr\n",
    "\n",
    "  # do the optimization\n",
    "  optimizer.step()\n",
    "\n",
    "  torch.cuda.synchronize() # needed b/c CPU schedules GPU kernels to run and then continues so acts as a block\n",
    "  t1 = time.time()\n",
    "  if master_process:\n",
    "    print(f\"step {step} loss {loss_accum.item():.6f} time {t1-t0} lr {lr:.4e} norm {norm:.4f}\")\n",
    "\n",
    "if ddp:\n",
    "  destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X3Egst6XMPRC"
   },
   "outputs": [],
   "source": [
    "if master_process:\n",
    "    torch.save(model.state_dict(), \"gpt_trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlGmAHYROEqy"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Tokenize the input prompt\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    input_ids = enc.encode(prompt)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move input to the correct device (GPU or CPU)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Start generating tokens\n",
    "    generated = input_ids\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass to get logits\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(generated)\n",
    "\n",
    "        # Take the last token logits, convert to probabilities\n",
    "        logits = logits[:, -1, :]\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample from the distribution (can use `torch.argmax` for greedy decoding)\n",
    "        next_token = torch.multinomial(probabilities, 1)\n",
    "\n",
    "        # Append the generated token to the input sequence\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    # Decode the generated tokens back into text\n",
    "    generated_text = enc.decode(generated[0].cpu().numpy())\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(model, prompt, max_length=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jZB9SSeZuv-",
    "outputId": "9bd824b1-00db-49c5-ebe9-e38f8210d1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Model saved to /content/drive/My Drive/gpt_trained_model.pth\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "save_path = \"/content/drive/My Drive/gpt_trained_model.pth\"  # Update path as needed\n",
    "if master_process:\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxrSlxb2cMHo",
    "outputId": "58db5adf-38ef-4640-c45e-512d19a1f8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ELy3jq7REXd",
    "outputId": "a2d0602f-947d-4c2c-b4b0-939113f4b02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (sample 0): What hath you say,\n",
      "\n",
      "A, I you to?T that and her:\n",
      "If to, myUS the.\n",
      "\n",
      "That my, I thy on, for he you and:\n",
      "O a, to;And of,\n",
      "To my?\n",
      "And's,\n",
      "H,\n",
      "With with for and:\n",
      "In!\n",
      " sir, for I this with?\n",
      "H?\n",
      "As not beIO!\n",
      "Of in.\n",
      "\n",
      "\n",
      "And\n",
      "\n",
      "What with,\n",
      "R\n",
      "With with me shall, but\n",
      "Th?\n",
      " sir:\n",
      "R, is him\n",
      "To\n",
      "My of'd be,\n",
      "\n",
      "Generated text (sample 1): What hath you say, this your so you,\n",
      "The her, I he with! I thy the:\n",
      "KING:\n",
      "The,\n",
      "What, the:What.First\n",
      "If,\n",
      "Why that\n",
      "No that with:\n",
      "For me your.\n",
      "O is it it by I my:\n",
      "This\n",
      "\n",
      "That and it, be not,\n",
      "The's a,\n",
      "HowIO.Now, thisEN is for be\n",
      "Now.\n",
      "I the all of the;\n",
      "\n",
      "O;\n",
      " I's that.\n",
      "What.\n",
      " but have.\n",
      "\n",
      "For for with his shall'\n",
      "\n",
      " for he:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# Define the evaluation loop for text generation\n",
    "def generate_text(model, enc, device, num_return_sequences=2, max_length=128, prompt=\"What hath you say,\"):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Encode the input prompt and prepare the input tensor\n",
    "    tokens = enc.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # Repeat for num_return_sequences\n",
    "    xgen = tokens.to(device)  # Move to the appropriate device (GPU/CPU)\n",
    "\n",
    "    # Create a random generator for sampling\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(42)  # Set a seed for reproducibility\n",
    "\n",
    "    # Generate tokens until reaching max_length\n",
    "    while xgen.size(1) < max_length:\n",
    "        with torch.no_grad():  # No gradient tracking during generation\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):  # Mixed precision (optional)\n",
    "                logits, _ = model(xgen)  # Get logits (B, T, vocab_size)\n",
    "\n",
    "            logits = logits[:, -1, :]  # Get logits for the last token (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1)  # Get probabilities for the next token\n",
    "\n",
    "            # Top-k sampling: Select the top 50 tokens\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "            # Sample from the top-k probabilities\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng)  # (B, 1)\n",
    "            xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "\n",
    "            # Append the sampled token to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    # Decode and print the generated sequences\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()  # Get the generated tokens for this sequence\n",
    "        decoded = enc.decode(tokens)  # Decode tokens back to text\n",
    "        print(f\"Generated text (sample {i}): {decoded}\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'model' is your trained model and 'enc' is your tokenizer\n",
    "\n",
    "# Move model to device (if not already done)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# Generate text from the model\n",
    "generate_text(model, enc, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEUXFfcZZHYr"
   },
   "outputs": [],
   "source": [
    "# # Inference\n",
    "# num_return_sequences = 5\n",
    "# max_length = 30\n",
    "\n",
    "# device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#   device = \"cuda\"\n",
    "\n",
    "# #model = GPT.from_pretrained('gpt2')\n",
    "# model = GPT(GPTConfig())\n",
    "# model.eval() # evaluation mode means not going to use any backtracking so it won't cache values\n",
    "# model.to(device) # move tensors to GPU\n",
    "\n",
    "# import tiktoken\n",
    "# enc = tiktoken.get_encoding(\"gpt2\") # gpt2 token encoding\n",
    "# tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "# tokens = torch.tensor(tokens, dtype=torch.long) # (8, )\n",
    "# tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "# x = tokens.to(device) # X is the idx that can be passed into forward to obtain logits\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "\n",
    "# while x.size(1) < max_length:\n",
    "#   # x is (B, T) w/ B = 5, T = 8\n",
    "#   with torch.no_grad():\n",
    "#     # logits of the next token\n",
    "#     logits = model(x) # (B, T, vocab_size)\n",
    "\n",
    "#     # get logits for the last position because thats the token that needs to be identified\n",
    "#     logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "#     # get the probabilites (use softmax)\n",
    "#     probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "\n",
    "#     # topk = 50 (hf default)\n",
    "#     topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1) # (B, 50)\n",
    "\n",
    "#     # select a token form topk_probs\n",
    "#     ix = torch.multinomial(topk_probs, 1) #(B, 1) (randomly select one from top 50)\n",
    "\n",
    "#     # gather corresponding indices\n",
    "#     xcol = torch.gather(topk_indices, -1, ix) # (B, 1) pick the ix token from top 50\n",
    "\n",
    "#     # append to the seq\n",
    "#     x = torch.cat((x, xcol), dim=1) # (B, T+1)  (add new token to the existing seq autoregressive)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#   tokens = x[i,:max_length].tolist() # get the tokens up to max_length for the batch idx\n",
    "#   decoded = enc.decode(tokens)\n",
    "#   print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
